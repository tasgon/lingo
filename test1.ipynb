{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the T5 tokenizer and models\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "english_to_lingo_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "lingo_to_english_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "embedding_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment that rewards Lingo translations\n",
    "class LingoTranslationEnvironment:\n",
    "    def __init__(self, tokenizer, english_to_lingo_model, lingo_to_english_model, embedding_model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.english_to_lingo_model = english_to_lingo_model\n",
    "        self.lingo_to_english_model = lingo_to_english_model\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def get_reward(self, english_text, lingo_text, back_to_english_text):\n",
    "        # Convert texts to embeddings\n",
    "        english_embedding = self._get_text_embedding(english_text)\n",
    "        back_to_english_embedding = self._get_text_embedding(back_to_english_text)\n",
    "\n",
    "        # Compute similarity between the original English text and back-to-English translation\n",
    "        similarity = F.cosine_similarity(\n",
    "            english_embedding.unsqueeze(0),\n",
    "            back_to_english_embedding.unsqueeze(0),\n",
    "        )\n",
    "\n",
    "        # Compute the reward based on translation similarity and Lingo token length\n",
    "        reward = similarity - 0.1 * len(lingo_text.split())\n",
    "\n",
    "        return reward.item()\n",
    "\n",
    "    def _get_text_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        # Create a tensor with the same shape as input_ids filled with the decoder_start_token_id\n",
    "        decoder_input_ids = torch.full_like(inputs[\"input_ids\"], self.embedding_model.config.decoder_start_token_id)\n",
    "\n",
    "        outputs = self.embedding_model(**inputs, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n",
    "        hidden_states = outputs.encoder_hidden_states[-1]  # Get the last hidden state\n",
    "        return hidden_states.mean(dim=1).squeeze()  # Average the token embeddings and remove the batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = LingoTranslationEnvironment(tokenizer, english_to_lingo_model, lingo_to_english_model, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent using Proximal Policy Optimization (PPO)\n",
    "def train_agent_ppo(english_texts, num_epochs, num_rollouts, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        rewards = []\n",
    "        for text in english_texts:\n",
    "            for _ in range(num_rollouts):\n",
    "                with torch.no_grad():\n",
    "                    # Generate Lingo translation\n",
    "                    english_input = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "                    lingo_translation = english_to_lingo_model.generate(english_input)\n",
    "                    lingo_text = tokenizer.decode(lingo_translation[0], skip_special_tokens=True)\n",
    "                    \n",
    "                    # Generate back-to-English translation\n",
    "                    lingo_input = tokenizer(lingo_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "                    back_to_english_tokens = lingo_to_english_model.generate(lingo_input)\n",
    "                    back_to_english_text = tokenizer.decode(back_to_english_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "                # Calculate the reward for this rollout\n",
    "                reward = env.get_reward(text, lingo_text, back_to_english_text)\n",
    "                rewards.append(reward)\n",
    "\n",
    "        # Compute the average reward\n",
    "        avg_reward = sum(rewards) / len(rewards)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}: Average reward = {avg_reward}\")\n",
    "\n",
    "        # Update the models using the PPO algorithm\n",
    "        optimizer.zero_grad()\n",
    "        loss = -torch.tensor(avg_reward, requires_grad=True)  # Minimize the negative reward\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shapor/src/lingo/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500: Average reward = 0.2387471248706182\n",
      "Epoch 2/500: Average reward = 0.2387471248706182\n"
     ]
    }
   ],
   "source": [
    "# Define the training data and optimizer\n",
    "english_texts = [\n",
    "    \"hello\",\n",
    "    \"world\",\n",
    "    \"how are you\",\n",
    "    \"goodbye\",\n",
    "    \"please\",\n",
    "    \"thank you\",\n",
    "    \"yes\",\n",
    "    \"no\",\n",
    "    \"good morning\",\n",
    "    \"good night\",\n",
    "    \"happy\",\n",
    "    \"sad\",\n",
    "]\n",
    "\n",
    "optimizer = Adam(list(english_to_lingo_model.parameters()) + list(lingo_to_english_model.parameters()), lr=5e-5)\n",
    "\n",
    "# Train the agent\n",
    "num_epochs = 500\n",
    "num_rollouts = 10\n",
    "train_agent_ppo(english_texts, num_epochs, num_rollouts, optimizer)\n",
    "\n",
    "# Save the models\n",
    "english_to_lingo_model.save_pretrained(\"english_to_lingo\")\n",
    "lingo_to_english_model.save_pretrained(\"lingo_to_english\")\n",
    "\n",
    "# Test the models\n",
    "test_english_texts = [\n",
    "    \"hello\",\n",
    "    \"how are you\",\n",
    "    \"goodbye\",\n",
    "    \"good morning\",\n",
    "]\n",
    "\n",
    "for text in test_english_texts:\n",
    "    # Generate Lingo translation\n",
    "    lingo_translation = english_to_lingo_model.generate(\n",
    "        **tokenizer(text, return_tensors=\"pt\")\n",
    "    )\n",
    "    lingo_text = tokenizer.decode(lingo_translation[0], skip_special_tokens=True)\n",
    "\n",
    "    # Generate back-to-English translation\n",
    "    back_to_english_translation = lingo_to_english_model.generate(\n",
    "        **tokenizer(lingo_text, return_tensors=\"pt\")\n",
    "    )\n",
    "    back_to_english_text = tokenizer.decode(back_to_english_translation[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Original English: {text}\")\n",
    "    print(f\"Lingo: {lingo_text}\")\n",
    "    print(f\"Back to English: {back_to_english_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
